# Grab-n-Go Microgesture Recognition Dataset
This repository provides the Grab-n-Go dataset, a collection of C-FMCW-based active acoustic sensing data for microgesture recognition while users are holding various objects.

**CHI-JUNG LEE**, Cornell University, USA (cl2358@cornell.edu, [ORCID: 0000-0002-1887-4000](https://orcid.org/0000-0002-1887-4000))  
**JIAXIN LI**, Cornell University, USA (jl2726@cornell.edu, [ORCID: 0009-0000-7635-6749](https://orcid.org/0009-0000-7635-6749))  
**TIANHONG CATHERINE YU**, Cornell University, USA (ty274@cornell.edu, [ORCID: 0000-0002-3742-0178](https://orcid.org/0000-0002-3742-0178))  
**RUIDONG ZHANG**, Cornell University, USA (rz379@cornell.edu, [ORCID: 0000-0001-8329-0522](https://orcid.org/0000-0001-8329-0522))  
**VIPIN GUNDA**, Cornell University, USA (vg245@cornell.edu, [ORCID: 0009-0000-5500-2183](https://orcid.org/0009-0000-5500-2183))  
**FRANÇOIS GUIMBRETIÈRE**, Cornell University, USA (fvg3@cornell.edu, [ORCID: 0000-0002-5510-6799](https://orcid.org/0000-0002-5510-6799))  
**CHENG ZHANG**, Cornell University, USA (chengzhang@cornell.edu, [ORCID: 0000-0002-5079-5927](https://orcid.org/0000-0002-5079-5927))  


## Table of Contents
1.  [Introduction](#1-introduction)
2.  [Data Collection Methodology](#2-data-collection-methodology)
    * [2.1 Sensing Modality and Device](#21-sensing-modality-and-device)
    * [2.2 Feature Representation](#22-feature-representation)
    * [2.3 Participants](#23-participants)
    * [2.4 Microgesture Set](#24-microgesture-set)
    * [2.5 Objects](#25-objects)
    * [2.6 Data Volume](#26-data-volume)
    * [2.7 Definition of a Session](#27-definition-of-a-session)
    * [2.8 Hardware, Software, and Protocols](#28-hardware-software-and-protocols)
3.  [Data Structure and Usage](#3-data-structure-and-usage)
    * [3.1 File Naming Conventions](#31-file-naming-conventions)
    * [3.2 Purpose of Files and Folders](#32-purpose-of-files-and-folders)
4. [Performance Notes](#4-performance-notes)
5. [License and Citation](#5-license-and-citation)
6. [Contact](#6-contact)

---

## 1. Introduction

The Grab-n-Go dataset supports research into always-available, subtle hand microgesture recognition in "hands-occupied" scenarios. It leverages a novel active acoustic sensing approach (C-FMCW) to capture rich information about hand movements, grasping poses, and object geometries using a single wrist-worn device. This dataset is designed to facilitate the development and evaluation of machine learning models for fine-grained interaction in everyday contexts.

## 2. Data Collection Methodology

### 2.1 Sensing Modality and Device

Data was collected using a custom-built, lightweight, watch-like wristband device employing **C-FMCW (Cross-Correlation-Based Frequency-Modulated Continuous Wave) active acoustic sensing**. The device transmits specific frequency sweeps (18-21 kHz and 21.5-24.5 kHz) and records the reflected signals.

### 2.2 Feature Representation

The raw acoustic signals are processed into **"echo profiles,"** which serve as the primary 2D feature maps for machine learning models. These echo profiles are generated by computing the cross-correlation between the transmitted and received acoustic signals, directly capturing physical object-hand relationships and reflection characteristics.

**Acronyms/Abbreviations:**
* **C-FMCW:** Correlation-based Frequency-Modulated Continuous Wave. A technique used for active acoustic sensing.
* **Echo Profile:** A 2D feature map representing the correlation strength of returned acoustic signals over time and distance.
* **.npy:** NumPy array file format, used for storing raw echo profile data.
* **.png:** Portable Network Graphics image file format, used for visualizing the echo profiles.

### 2.3 Participants

The dataset comprises data from a total of **18 participants**:
* **Initial Study:** 10 participants (P1-P7, P11-P13).
* **Follow-up Study:** An additional 8 participants (P14-P21).

### 2.4 Microgesture Set

The dataset includes **30 distinct microgestures**, with 6 microgestures for 5 grasping poses based on Schlesinger's grasp taxonomy. These microgestures represent subtle hand movements performed while holding objects.

### 2.5 Objects

Data was collected across a diverse set of **35 distinct objects**:

* **Non-Deformable Objects (from Initial Study - 24 objects):**

![image](figures/objects-gestures.png)


* **Deformable Objects (from Follow-up Study - 10 objects)**

![image](figures/study2-objects.png)

#### Participant-Object Assignments (Initial Study)
The table below details which 2 objects each participant interacted with for each of the 5 grasping poses in the initial study. Each row represents a participant (P1-P7, P11-P13), and columns 1-10 represent the objects assigned across the 5 grasping poses (2 objects per pose).

| Participant | Cylindrical 1 | Cylindrical 2 | Hook 1 | Hook 2 | Tip 1 | Tip 2 | Palmar 1 | Palmar 2 | Spherical 1 | Spherical 2 | 
|-------------|----------|----------|----------|----------|----------|----------|----------|----------|----------|-----------|
| P1          | Pen Holder | Soda Can | Suitcase | Tool Box | Marker | Crochet Hook | Tall Cardboard Box | Basket | Gotcha Ball | Jar Lid |
| P2          | Soda Can | Paper Cup | Tool Box | Yoga Mat Holder | Crochet Hook | Pen | Basket | Closet Organizer | Jar Lid | Desktop Vacuum |
| P3          | Paper Cup | Lotion Box | Yoga Mat Holder | Lunch Bag | Pen | Glue Stick | Closet Organizer | Paper Bag | Desktop Vacuum | Cleaning Brush |
| P4          | Lotion Box | Beer Bottle | Lunch Bag | Double Strap Bag | Glue Stick | Probe | Paper Bag | Long Cardboard Box | Cleaning Brush | Timer |
| P5          | Beer Bottle | Pen Holder | Double Strap Bag | Suitcase | Probe | Marker | Long Cardboard Box | Tall Cardboard Box | Timer | Gotcha Ball |
| P6          | Beer Bottle | Soda Can | Double Strap Bag | Tool Box | Probe | Crochet Hook | Long Cardboard Box | Basket | Timer | Jar Lid |
| P7          | Paper Cup | Pen Holder | Suitcase | Yoga Mat Holder | Pen | Marker | Closet Organizer | Tall Cardboard Box | Gotcha Ball | Desktop Vacuum |
| P11          | Soda Can | Lotion Box | Lunch Bag | Tool Box | Crochet Hook | Glue Stick | Paper Bag | Basket | Jar Lid | Cleaning Brush |
| P12          | Paper Cup | Beer Bottle | Yoga Mat Holder | Double Strap Bag | Pen | Probe | Long Cardboard Box | Closet Organizer | Desktop Vacuum | Timer |
| P13         | Pen Holder | Lotion Box | Lunch Bag | Suitcase | Marker | Glue Stick | Tall Cardboard Box | Paper Bag | Gotcha Ball | Cleaning Brush |

#### Participant-Object Assignments (Follow-Up Study)

| Participant | Cylindrical | Hook | Tip | Palmar | Spherical |
|---|---|---|---|---|---|
| P14 | Laminated Foil Pouch | Phone Strap | DuPont Wire | Throw Pillow | Loofah |
| P15 | Laminated Foil Pouch | Phone Strap | DuPont Wire | Throw Pillow | Loofah |
| P16 | Laminated Foil Pouch | Phone Strap | DuPont Wire | Throw Pillow | Loofah |
| P17 | Laminated Foil Pouch | Phone Strap | DuPont Wire | Throw Pillow | Loofah |
| P18 | Cracker Bag | Empty Bag | Weaving Needle | Cushion | Sponge |
| P19 | Cracker Bag | Empty Bag | Weaving Needle | Cushion | Sponge |
| P20 | Cracker Bag | Empty Bag | Weaving Needle | Cushion | Sponge |
| P21 | Cracker Bag | Empty Bag | Weaving Needle | Cushion | Sponge |

### 2.6 Data Volume

The complete dataset contains a total of **20,160 microgesture instances**. Each instance includes the raw original and differential echo profile in .npy format and visualization in .png format.

### 2.7 Definition of a Session

A "session" refers to a distinct, continuous block of data collection for a participant. During a session, a participant performs a set of microgestures with assigned objects. Between each session, the participant removed and re-weared the device. In the context of our training and evaluation, data from different sessions were used to assess model generalization over time and wearing changes. For example, in some evaluations, models were trained on the first five sessions and validated on the final session for each participant.

### 2.8 Hardware and Software

![image](figures/prototype.png)

* **Hardware:** Custom-built wristband device with integrated speaker and microphone.
  * **Sensors:** Two speaker-microphone pairs (OWR-06944T-16B speakers and ICS-43434 microphones) mounted on customized PCBs.
  * **Microcontroller:** Sensors are connected to a customized microcontroller module (including an SGW1110 module and an MAX98357A audio amplifier) via FPC ribbons.

* **Instrument Settings:**

  * **Transmitted Signal:** FMCW sweeps between 18-21 kHz (Speaker 1) and 21.5-24.5 kHz (Speaker 2).

  * **Sweep Duration:** 12 ms per frequency sweep.

  * **Sampling Rate:** 50 kHz.

  * **Filtering:** Band-pass filter applied to received signals before cross-correlation.

* **Software:** Data acquisition and processing were performed using Python. 
  * **Signal Processing (Echo Profile Generation):**
    * **Signal Filtering:** Received signals are filtered to keep only the frequencies of our transmitted signals (18-21 kHz and 21.5-24.5 kHz) ([scipy.signal.butter](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html))
    * **Core Method:** Performs **cross-correlation** between the transmitted FMCW signal and the band-pass filtered received signals to extract signal strengths at different return times. ([np.correlate](https://numpy.org/doc/stable/reference/generated/numpy.correlate.html))
    * **Time-to-Distance Mapping:** Time-domain results are mapped into the distance domain (using the known speed of sound) to generate the **echo frames**. An echo frame is 1-pixel wide (equivalent to 12 ms, the duration of one frequency sweep) and 600-pixels long (equivalent to a 2.06 m sensing range).
    * **Original Echo Profiles:** Created by stacking the 1-pixel wide echo frames along the time (x-axis), providing continuous reflection strengths at different distances. This captures static hand geometries.
    * **Differential Echo Profiles:** Calculated by subtracting the previous echo frame from the current echo frame. This amplifies the hand geometry movements.

### 2.9 Protocols

For each grasping pose in the order of Cylindrical, Hook, Tip, Palmar, and Spherical, each participant collected 6 sessions of data. During each session, the participant first performed 4 repetitions of the 6 microgestures in a randomized order, using one of the assigned objects within the current grasping pose category. Each microgesture was performed in a 2-second window. In the initial study, this process was repeated with the second assigned object. Between each session, the participant removed and re-wore the device under the researcher’s guidance. 

## 3. Data Structure and Usage

The dataset is organized into a clear directory structure.

### 3.1 File Naming Conventions

* **Participant Data:** `PXX/` (where `XX` is the participant ID, e.g., `P1`, `P2`, up to `P21`)

* **Microgesture Instance Files:** Within each participant folder (`PXX/`), individual microgesture samples are stored directly.

  * **File Name Format:** `PXX_sessionY_graspname_gesturename_instanceZ_type.extension`

    * `PXX`: Participant ID (e.g., `P1`).

    * `sessionY`: Session number (e.g., `session1`).

    * `graspname`: The name of the grasp used in that instance (e.g., `cylindrical`).

    * `gesturename`: The name of the microgesture performed (e.g., `middle-tap`).

    * `instanceZ`: The instance number for that specific microgesture, object, and session (e.g., `1`).

    * `type`: Indicates the type of echo profile:

      * `original`: The raw echo profile data.

      * `diff`: A differential echo profile (e.g., representing changes).

    * `extension`: File format, either `.npy` (for raw numerical data) or `.png` (for visual representation).

  * **Example Filenames:**

    * `P1_session1_cylindrical_middle-tap_1_diff.npy`

    * `P1_session1_cylindrical_middle-tap_1_original.png`

### 3.2 Purpose of Files and Folders

* **`PXX/` folders:** Each folder contains all data collected from a specific participant across all sessions and objects they interacted with.

* **Microgesture Instance Files (`.npy`):** These are the core data files containing the 2D echo profiles (as NumPy arrays) for each individual microgesture execution. These are the primary input for machine learning models.

* **Microgesture Instance Visualizations (`.png`):** These files provide visual representations of the echo profiles, useful for quick inspection and understanding of the data.

## 4. Performance Notes

The associated research paper demonstrates:
* **Machine Learning Framework:** Models were implemented using **PyTorch**.
    * **Model Architecture:** A customized **Encoder-Decoder model architecture**.
        * **Encoder:** **ResNet-18** as the encoder backbone.
        * **Decoder:** Includes an adaptive 2D average pooling layer (output size \[1, 1\]), a dropout layer (rate 0.6) to prevent overfitting, and a fully connected layer with an output dimension of 30 for classification.
        * **Input Tensor:** Size **155 × 70 × 8**, representing:
            * 1.8 seconds of temporal data (155 pixels along the time axis).
            * A 24 cm range of interest (70 pixels along the distance axis).
            * 8 stacked channels (four original echo profiles and their corresponding four differential echo profiles).
        * **Optimization:** **Cross-entropy (CE) loss** is used as the optimization objective.
        * **Hyperparameters:** Initial learning rate of **0.0002** and a batch size of **8**.
    * **Data Augmentation:** Applied during training to address variations in hand sizes and device positioning:
        * **Vertical Shifting:** Echo profiles were randomly shifted vertically by up to 6 pixels.
        * **Amplitude Jitter:** In 80% of training iterations, each pixel’s intensity value was multiplied by a random factor between 0.95 and 1.05.

* **Results:**
  * An average recognition accuracy of 92.0% with non-deformable objects (initial study).

  * An average recognition accuracy of 92.9% with deformable objects, and 95.0% for non-deformable objects when evaluated on the combined dataset.

## 5. License and Citation

This dataset is shared under a Creative Commons 1.0 Universal Public Domain Dedication (https://creativecommons.org/publicdomain/zero/1.0/). The material can be copied, modified and used without permission, but attribution to the original authors is always appreciated. 

```
@misc{lee2025grabngo,
  author = {Chi-Jung Lee and Jiaxin Li and Tianhong Yu and Ruidong Zhang and Vipin Gunda and François Guimbretière and Cheng Zhang},
  title = {Data from: Grab-n-Go: On-the-Go Microgesture Recognition with Objects in Hand},
  year         = {2025},
  publisher    = {Cornell University Library eCommons Repository},
  url = {\url{https://doi.org/10.7298/7kbd-vv75}},
  doi          = {10.7298/7kbd-vv75},
  note         = {[dataset]}
}

```

Please cite the associated paper if you use this dataset in your research:
```
@article{10.1145/3699741,
  author = {Lee, Chi-Jung and Li, Jiaxin and Yu, Tianhong Catherine and Zhang, Ruidong and Gunda, Vipin Guimbreti\`{e}re, Fran\c{c}ois and Zhang, Cheng},
  title = {Grab-n-Go: On-the-Go Microgesture Recognition with Objects in
  Hand},
  year = {2025},
  issue_date = {September 2025},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {9},
  number = {3},
  url = {https://doi.org/10.1145/3749469},
  doi = {10.1145/3749469},
  abstract = {As computing devices become increasingly integrated into daily life, there is a growing need for intuitive, always-available interaction methods --- even when users’ hands are occupied. In this paper, we introduce Grab-n-Go, the first wearable device that leverages active acoustic sensing to recognize subtle hand microgestures while holding various objects. Unlike prior systems that focus solely on free-hand gestures or basic hand-object activity recognition, Grab-n-Go simultaneously captures information about hand microgestures, grasping poses, and object geometries using a single wristband, enabling the recognition of fine-grained hand movements occurring within activities involving occupied hands. A deep learning framework processes these complex signals to identify 30 distinct microgestures, with 6 microgestures for each of the 5 grasping poses. In a user study with 10 participants and 25 everyday objects, Grab-n-Go achieved an average recognition accuracy of 92.0%. A follow-up study further validated Grab-n-Go's robustness against 10 more challenging, deformable objects. These results underscore the potential of Grab-n-Go to provide seamless, unobtrusive interactions without requiring modifications to existing objects. The complete dataset, comprising data from 18 participants performing 30 microgestures with 35 distinct objects, is publicly available at https://github.com/cjlisalee/Grab-n-Go_Data with the DOI: https://doi.org/10.7298/7kbd-vv75.},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  month = sep,
  articleno = {99},
  numpages = {27}
}
```