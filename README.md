# Grab-n-Go Microgesture Recognition Dataset

This repository provides the Grab-n-Go dataset, a collection of C-FMCW-based active acoustic sensing data for microgesture recognition while users are holding various objects.

## Table of Contents
1.  [Introduction](#1-introduction)
2.  [Data Collection Methodology](#2-data-collection-methodology)
    * [2.1 Sensing Modality and Device](#21-sensing-modality-and-device)
    * [2.2 Feature Representation](#22-feature-representation)
    * [2.3 Participants](#23-participants)
    * [2.4 Microgesture Set](#24-microgesture-set)
    * [2.5 Objects](#25-objects)
    * [2.6 Data Volume](#26-data-volume)
    * [2.7 Definition of a Session](#27-definition-of-a-session)
    * [2.8 Hardware, Software, and Protocols](#28-hardware-software-and-protocols)
3.  [Data Structure and Usage](#3-data-structure-and-usage)
    * [3.1 File Naming Conventions](#31-file-naming-conventions)
    * [3.2 Purpose of Files and Folders](#32-purpose-of-files-and-folders)
4. [Performance Notes](#4-performance-notes)
5. [License and Citation](#5-license-and-citation)
6. [Contact](#6-contact)

---

## 1. Introduction

The Grab-n-Go dataset supports research into always-available, subtle hand microgesture recognition in "hands-occupied" scenarios. It leverages a novel active acoustic sensing approach (C-FMCW) to capture rich information about hand movements, grasping poses, and object geometries using a single wrist-worn device. This dataset is designed to facilitate the development and evaluation of machine learning models for fine-grained interaction in everyday contexts.

## 2. Data Collection Methodology

### 2.1 Sensing Modality and Device

Data was collected using a custom-built, lightweight, watch-like wristband device employing **C-FMCW (Frequency-Modulated Continuous Wave) active acoustic sensing**. The device transmits specific frequency sweeps (18-21 kHz and 21.5-24.5 kHz) and records the reflected signals.

### 2.2 Feature Representation

The raw acoustic signals are processed into **"echo profiles,"** which serve as the primary 2D feature maps for machine learning models. These echo profiles are generated by computing the cross-correlation between the transmitted and received acoustic signals, directly capturing physical object-hand relationships and reflection characteristics.

**Acronyms/Abbreviations:**
* **C-FMCW:** Correlation-based Frequency-Modulated Continuous Wave. A technique used for active acoustic sensing.
* **Echo Profile:** A 2D feature map representing the correlation strength of returned acoustic signals over time and distance.
* **.npy:** NumPy array file format, used for storing raw echo profile data.
* **.png:** Portable Network Graphics image file format, used for visualizing the echo profiles.

### 2.3 Participants

The dataset comprises data from a total of **18 participants**:
* **Initial Study:** 10 participants (P1-P7, P11-P13).
* **Follow-up Study:** An additional 8 participants (P14-P21).

### 2.4 Microgesture Set

The dataset includes **30 distinct microgestures**, organized into 5 grasping poses based on Schlesinger's grasp taxonomy. These microgestures represent subtle hand movements performed while holding objects.

### 2.5 Objects

Data was collected across a diverse set of **35 distinct objects**:

* **Non-Deformable Objects (from Initial Study - 24 objects):**

![image](figures/objects-gestures.png)


* **Deformable Objects (from Follow-up Study - 10 objects)**

![image](figures/study2-objects.png)

#### Participant-Object Assignments (Initial Study)
The table below details which 2 objects each participant interacted with for each of the 5 grasping poses in the initial study. Each row represents a participant (P1-P7, P11-P13), and columns 1-10 represent the objects assigned across the 5 grasping poses (2 objects per pose).

| Participant | Object 1 | Object 2 | Object 3 | Object 4 | Object 5 | Object 6 | Object 7 | Object 8 | Object 9 | Object 10 |
|-------------|----------|----------|----------|----------|----------|----------|----------|----------|----------|-----------|
| P1          | Pen Holder | Soda Can | Suitcase | Tool Box | Marker | Crochet Hook | Tall Cardboard Box | Basket | Gotcha Ball | Jar Lid |
| P2          | Soda Can | Paper Cup | Tool Box | Yoga Mat Holder | Crochet Hook | Pen | Basket | Closet Organizer | Jar Lid | Desktop Vacuum |
| P3          | Paper Cup | Lotion Box | Yoga Mat Holder | Lunch Bag | Pen | Glue Stick | Closet Organizer | Paper Bag | Desktop Vacuum | Cleaning Brush |
| P4          | Lotion Box | Beer Bottle | Lunch Bag | Double Strap Bag | Glue Stick | Probe | Paper Bag | Long Cardboard Box | Cleaning Brush | Timer |
| P5          | Beer Bottle | Pen Holder | Double Strap Bag | Suitcase | Probe | Marker | Long Cardboard Box | Tall Cardboard Box | Timer | Gotcha Ball |
| P6          | Beer Bottle | Soda Can | Double Strap Bag | Tool Box | Probe | Crochet Hook | Long Cardboard Box | Basket | Timer | Jar Lid |
| P7          | Paper Cup | Pen Holder | Suitcase | Yoga Mat Holder | Pen | Marker | Closet Organizer | Tall Cardboard Box | Gotcha Ball | Desktop Vacuum |
| P11          | Soda Can | Lotion Box | Lunch Bag | Tool Box | Crochet Hook | Glue Stick | Paper Bag | Basket | Jar Lid | Cleaning Brush |
| P12          | Paper Cup | Beer Bottle | Yoga Mat Holder | Double Strap Bag | Pen | Probe | Long Cardboard Box | Closet Organizer | Desktop Vacuum | Timer |
| P13         | Pen Holder | Lotion Box | Lunch Bag | Suitcase | Marker | Glue Stick | Tall Cardboard Box | Paper Bag | Gotcha Ball | Cleaning Brush |

### 2.6 Data Volume

The complete dataset contains a total of **20,160 microgesture instances**. Each instance includes the raw original and differential echo profile in .npy format and visualization in .png format.

### 2.7 Definition of a Session

A "session" refers to a distinct, continuous block of data collection for a participant. During a session, a participant performs a set of microgestures with assigned objects. Between each session, the participant removed and re-weared the device. In the context of our training and evaluation, data from different sessions were used to assess model generalization over time and wearing changes. For example, in some evaluations, models were trained on the first five sessions and validated on the final session for each participant.

### 2.8 Hardware, Software, and Protocols

* **Hardware:** Custom-built wristband device with integrated speaker and microphone.

* **Software:** Data acquisition and processing were performed using Python. Machine learning models were implemented using PyTorch.

* **Instrument Settings:**

  * **Transmitted Signal:** FMCW sweeps between 18-21 kHz (Speaker 1) and 21.5-24.5 kHz (Speaker 2).

  * **Sweep Duration:** 12 ms per frequency sweep.

  * **Sampling Rate:** 50 kHz.

  * **Filtering:** Band-pass filter applied to received signals before cross-correlation.

* **Protocols:** For each grasping pose in the order of Cylindrical, Hook, Tip, Palmar, and Spherical, each participant collected 6 sessions of data. During each session, the participant first performed 4 repetitions of the 6 microgestures in a randomized order, using one of the assigned objects within the current grasping pose category. Each microgesture was performed in a 2-second window. Then, this process was repeated with the second assigned object. Between each session, the participant removed and re-weared the device under the researcher’s guidance. In total, there were 2 (objects) × 5 (grasping poses) × 6 (sessions) × 6 (microgestures) × 4 (repetitions) = 1440 microgesture instances collected from each participant in the initial study.

## 3. Data Structure and Usage

The dataset is organized into a clear directory structure.

### 3.1 File Naming Conventions

* **Participant Data:** `PXX/` (where `XX` is the participant ID, e.g., `P1`, `P2`, up to `P21`)

* **Microgesture Instance Files:** Within each participant folder (`PXX/`), individual microgesture samples are stored directly.

  * **File Name Format:** `PXX_sessionY_graspname_gesturename_instanceZ_type.extension`

    * `PXX`: Participant ID (e.g., `P1`).

    * `sessionY`: Session number (e.g., `session1`).

    * `graspname`: The name of the grasp used in that instance (e.g., `cylindrical`).

    * `gesturename`: The name of the microgesture performed (e.g., `middle-tap`).

    * `instanceZ`: The instance number for that specific microgesture, object, and session (e.g., `1`).

    * `type`: Indicates the type of echo profile:

      * `original`: The raw echo profile data.

      * `diff`: A differential echo profile (e.g., representing changes).

    * `extension`: File format, either `.npy` (for raw numerical data) or `.png` (for visual representation).

  * **Example Filenames:**

    * `P1_session1_cylindrical_middle-tap_1_diff.npy`

    * `P1_session1_cylindrical_middle-tap_1_original.png`

### 3.2 Purpose of Files and Folders

* **`PXX/` folders:** Each folder contains all data collected from a specific participant across all sessions and objects they interacted with.

* **Microgesture Instance Files (`.npy`):** These are the core data files containing the 2D echo profiles (as NumPy arrays) for each individual microgesture execution. These are the primary input for machine learning models.

* **Microgesture Instance Visualizations (`.png`):** These files provide visual representations of the echo profiles, useful for quick inspection and understanding of the data.

## 4. Performance Notes

The associated research paper demonstrates:

* An average recognition accuracy of 92.0% with non-deformable objects (initial study).

* An average recognition accuracy of 92.9% with deformable objects, and 95.0% for non-deformable objects when evaluated on the combined dataset.

* Low false-positive rates (0.2-0.3%), indicating high reliability.

* Evidence of overfitting (100% training accuracy vs. 92% testing accuracy) which is discussed in the paper as expected given dataset constraints and task complexity.

## 5. License and Citation

[To be added upon final publication]

Please cite the associated paper (details to be provided) if you use this dataset in your research:


## 6. Contact

For any questions or further information, please refer to the contact details in the associated publication or reach out to:

Chi-Jung Lee
cl2358@cornell.edu
SciFi Lab @ Cornell University